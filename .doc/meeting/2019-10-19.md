# Introduce

> 确定服务器配置、需求、时间人员安排



### 服务器

- v1版本服务器：3台 8G 2核

- 在v1版本的基础不变的情况下，希望master节点需要大一点，若可动态调配cpu内核建议为8G 4核

  > master 节点去年cpu很容易就拉满

- 从11月份到明年毕设结束，最晚估计为7月份



### 云数据库

- mysql
- mongo



### 系统架构

<img src="https://user-images.githubusercontent.com/17808702/67137427-bc869280-f267-11e9-8931-2efecfcc4d23.png" alt="image" style="zoom: 50%;" />

### 需求确定

- 项目管理：整个平台的信息管理中心
  - gitlab地址
  - docker信息
  - 集群信息
  - API信息（可做入口）
  - 监控信息（可做入口）
  - 全流程日志信息（可做入口）

- 代码管理：平台代码管理仓库
- 流水线管理
- 监控管理
- 权限管理
- 日志管理：

  - 日志来源
  - 日志时间
  - 日志分类

- 测试管理

- 监控管理：采用外围监控的方式进行**ISTIO**、**Promethus**
  - 监控告警
  - 链路分析

- API管理
  - 展示
    - 请求数量
    - 吞吐量
    - 响应时间
    - 目前状态
  - 策略：
    - 熔断
    - 限流
    - ...



坑：自动触发

### 监控模块
- 监控数据
  - 基础数据：
    当前平台的网关 API、当前服务数、当前实例数、中间件；
  - 外部请求数：显示24h 内有多少请求访问网关，及访问后的状态码分析和满意度（平均响应时间）统计； 
  - 外部IP 数：显示24h 内来源的IP 数及每个事件刻度下IP 数； 
  - 平台告警：显示24h 内告警总次数；
  - 所属服务：当前实例属于哪一个服务的运行体； 
  - 实例名称：实例的instanceId 信息； 
  - 实例状态：实例的status ，包含：UP、DOWN； 
  - 运行时长：当前时间距离实例注册开始时间的时间差； 
  - 内存使用：已使用的内存/被分配的内存；
<img width="1439" alt="1" src="https://user-images.githubusercontent.com/17808702/64095408-4ac6b800-cd91-11e9-9215-66c5836a2bce.png">

- 难点
  - 两种istio多集群链接方式都需要**对用户集群进行istio配置**
![TIM截图20190903100705](https://user-images.githubusercontent.com/17808702/64139232-ba40b400-ce32-11e9-813f-6546b10ee7cc.png)
![TIM截图20190903100722](https://user-images.githubusercontent.com/17808702/64139233-ba40b400-ce32-11e9-90ae-4d7478aab5be.png)
  - 采用promethus方式需要对**用户代码**进行更改，且不能进行istio api网关层面的管控
- 待决定
  - Prometheus支持自定义指标，但自定义指标可能会影响自带指标的使用
  - 第一步暂时不对用户代码进行埋点，而是进行外围监控
- 实施计划
  - 完善v1版本项目管理与应用管理模块
  - 接入开源监控模块工具
  - 实现看板实时监控与展示，拓展功能

### 告警模块
- 难点
  - 由于应用会被拆分成多个微服务，导致监控数据的爆炸增长，运维人员无法快速处理和展示这些监控数据。
  - 微服务之间的调用关系，导致出现问题时，故障排查很难。
  - 如何通过一个统一的监控系统纳管所有的需求。

- 告警规则配置参数
  - 指标：需要告警的指标信息，详情请参见**告警指标列表**
  - 条件：对监控的“指标”值的判断条件，包括：>、<、=、>=、<=;
  - 阈值：“指标”根据告警条件触发告警事件的临界值；
  - 统计周期：统计多少时间范围内的指标·数据；
  - 重复次数：“指标”达到“阈值”发生了多少次，由于会因为网络原因，会出现抖动情况，可以配置允许多次达到阈值之后才进行告警事件通知。
  - 状态：配置策略的执行与暂停执行；
  - 告警级别：策略重要性标示，包括：一般、严重；
- 配置具体告警实例
  - 接收人：发生告警事件后需要通知的人员信息；
  - 通知方式：邮件
  - 通知间隔：发生告警事件后，如果再次发生相同规则的告警时间，在通知间隔的时间内不再持续通知，避免通知重复，影响造成重要信息查看；
- 实施计划
  - 接入告警模块，配置告警规则及模板
  - 增强告警模块可用性和健壮性
- 告警配置具体步骤
<img width="1439" alt="1" src="https://user-images.githubusercontent.com/17808702/64095953-e6a4f380-cd92-11e9-8783-89ca2cd7ddb3.png">
<img width="1200" alt="2" src="https://user-images.githubusercontent.com/17808702/64095955-e73d8a00-cd92-11e9-891a-60c9b2f727b9.png">
<img width="1195" alt="3" src="https://user-images.githubusercontent.com/17808702/64095956-e73d8a00-cd92-11e9-9dec-8542a5df8cc8.png">
<img width="1199" alt="4" src="https://user-images.githubusercontent.com/17808702/64095957-e7d62080-cd92-11e9-83ef-85b23a77c15a.png">
<img width="1196" alt="5" src="https://user-images.githubusercontent.com/17808702/64095958-e7d62080-cd92-11e9-9487-489227626fee.png">
<img width="1166" alt="6" src="https://user-images.githubusercontent.com/17808702/64095959-e86eb700-cd92-11e9-844a-3b21ca2eb4c5.png">

  
### 链路分析模块
- 难点
  - 链路追踪显示所有服务之间的调用详情数据，通过 TraceId 记录每一个线程的完整链路数据
  - 相关应用服务器内部的线程处理模型、服务的线程处理模型
  - 记录经过的 Span 数、请求开始时间、耗时等详细信息
  - 展示完整的链路下钻及错误日志信息帮助使用者进行错误的追踪和精确定位
- 链路展现形式
  - 列表
    - 列表显示单次请求的入口方法；
    - 列表显示单次请求的状态为成功或者失败；
    - 列表展示单次请求花费的总时间；
    - 列表显示本次请求的调用方式，分为：gRPC、服务、消息、数据；
    - 列表展示单次请求进入时间；

<img width="1166" alt="7" src="https://github.com/552000264/user-images/blob/master/%E9%93%BE%E8%B7%AF%E5%88%97%E8%A1%A8%E5%B1%95%E7%8E%B0.png">


  - 搜索：界面可进行追踪数据搜索；
    - 服务：展示已接入追踪的所有服务数据，快捷找到请求入口；
    - 请求入口：请求开始访问的入口地址；
    - 请求状态：搜索调用成功或失败的链路；
    - 调用方式：用于搜索请求来源方式，快速了解东西流向，南北流量，数据库，消息等调用情况；
    - 持续时间：可搜索一段时间内的持续时间对应的调用链；
    - 时间：可根据时间搜索请求的进入时间数据；
    - 线程ID：全局唯一标示此条请求数据（TraceID）；
    - 排序：追踪列表默认根据请求时间倒序排列，通过搜索可根据请求消耗时间排序，找到耗时严重请求；
    
<img width="1166" alt="8" src="https://github.com/552000264/user-images/blob/master/%E9%93%BE%E8%B7%AF%E6%90%9C%E7%B4%A2.png">


- 链路图详情
  - 顶部ID：本条链路的TraceID，全局唯一；
  - 开始时间：链路入口时间节点；
  - 持续时间：整条链路的耗时；
  - Span 数据：本条链路经过多少个接口或方法数；
  - 链路中经过的服务数；
  - 经过的span 名称和调用方式（如：http-SpringMVC），如果经过的次span 响应错误，本行“❌”标记；
  - 选中任意一条跨度（Span），可查看到该跨度详细信息：包括跨度类型、组件名称、对方主机、是否有出错等信息。

<img width="1166" alt="9" src="https://github.com/552000264/user-images/blob/master/%E9%93%BE%E8%B7%AF%E8%AF%A6%E6%83%851.png">
<img width="1166" alt="10" src="https://github.com/552000264/user-images/blob/master/%E9%93%BE%E8%B7%AF%E8%AF%A6%E6%83%852.png">



### API网关模块
####  简介
API网关模块主要希望实现，对内部API网关的展示和策略调整。用户可以在这个模块中查看所有API，及每个API的流量情况，峰值等。同时还可以在这个模块中调整每个API的策略，达到限流、熔断等功能。

#### 解决方案

- 服务网格（ISTIO）

    在集群中部署ISTIO，能够完成对Kubernetes集群路由的集中控制，包括路由分发（可以完成多版本同时部署，完成灰度发布、金丝雀发布等功能），速率限制，设置超时等多个功能。
      
    要在集群中部署ISTIO，需要先检查集群是否有部署ISTIO的条件，若集群可以部署ISTIO，则为用户安装ISTIO。若集群存在各种问题，无法安装ISTIO，则需要让用户排错（optional：自动排错，继续安装）后安装ISTIO。
  使用ISTIO自带的GRAFANA可以查看多种数据，包括ISTIO自身

  **难点**
  - ISTIO的部署配置
  - 集群环境的检测（是否可以成功安装ISTIO）
  - 需要设定默认配置文件
  - 支持用户自定义配置（OPTIONAL）
  
  **问题**
  
  ISTIO虽然提供了非常强大的功能，但并没有为入口网关提供比较完善的解决方案。最原始版本的ISTIO默认使用K8S的ingress作为流量入口，但存在两个问题：
  - K8s Ingress是独立在Istio体系之外的，需要单独采用Ingress rule进行配置，导致系统入口和内部存在两套互相独立的路由规则配置，运维和管理较为复杂。

  - K8s Ingress rule的功能较弱，不能在入口处实现和网格内部类似的路由规则，也不具备网格sidecar的其它能力，导致难以从整体上为应用系统实现灰度发布、分布式跟踪等服务管控功能。

  0.8版本开始，社区使用Istio gateway代替K8Singress来表示流量入口Istio Gateway资源本身只能配置L4-L6的功能，例如暴露的端口，TLS设置等；但Gateway可以和绑定一个VirtualService，在VirtualService 中可以配置七层路由规则，这些七层路由规则包括根据按照服务版本对请求进行导流，故障注入，HTTP重定向，HTTP重写等所有Mesh内部支持的路由规则。

  然而除了上方这些功能外，采用Gateway和VirtualService实现的Istio Ingress Gateway提供了网络入口处的基础通信功能，包括可靠的通信和灵活的路由规则。但对于一个服务化应用来说，网络入口除了基础的通讯功能之外，还有一些其他的应用层功能需求，例如：

  - 第三方系统对API的访问控制

  - 用户对系统的访问控制

  - 修改请求/返回数据

  - 服务API的生命周期管理

  - 服务访问的SLA、限流及计费


  下图分别展示了Ingress、gateway和API GATEWAY作为流量入口的不同功能。
  <img width="1116" alt="7" src="https://zhaohuabing.com/img/2018-12-27-the-obstacles-to-put-istio-into-production/ingress-comparation.png">
 
  采用API网关+ISTIO能够提供更全面、强大的功能。
  <img width="1166" alt="8" src=" https://zhaohuabing.com/img/2018-12-27-the-obstacles-to-put-istio-into-production/api-gateway-and-envoy.png">
- API网关

  主流的开源解决方案有Kong、Zull等
  关于API GATEWAY与ISTIO结合这部分网络资源非常空缺，有待进一步探索

  **难点**
  - ISTIO与API GATEWAY的结合非常复杂，需要对ISTIO的内部结构非常熟悉，还需要对各类API GATEWAY有所了解。
  - 在上述基础上对两者进行结合，部署
  - 同时在客户集群上的安装也是个问题，用户很有可能安装各种代理，会产生各种冲突

### 日志模块

本模块主要负责收集并展示用户在使用集群过程中的所有日志信息。目前我们没有为用户提供创建集群的功能，使用的是在用户的集群中装上dop框架这样的方式来进行管理。Kubernetes上的日志管理工具主要是通过现有工具收集、处理日志进行，所以我们需要为用户提供在集群上安装/升级日志工具的功能，同时还需要考虑已有其他日志工具/日志工具版本太老等多种情况。

#### 简介
Kubernetes本身并没有为日志数据提供原生的存储解决方案，但可以将许多现有的日志记录解决方案集成到Kubernetes集群中。在Kubernetes中，有三个层次的日志：
- 基础日志
  
      基础日志使用 kubectl logs 命令即可将日志输出到标准输出流中
- Node级别的日志

      容器化应用中写入到stdout和stderr这两个流的内容一般是由NODE引擎处理和重定向的，docker就会重定向到日志记录驱动，在Kubernetes中该日志驱动被配置为以json格式写入文件。
- 群集级别的日志架构

      Kubernetes本身没有为群集级别日志记录提供原生解决方案，但有几种常见的方法可以采用：
      - 使用运行在每个Node上的Node级别的日志记录代理；
      - 在应用Pod中包含一个用于日志记录的sidecar。
      - 将日志直接从应用内推到后端。
      
  <img width="1166" alt="9" src="
https://www.kubernetes.org.cn/img/2018/07/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180709162118.png">
  我们所期望的日志管理模块对应的是这边集群级别的日志管理，即一个日志管理模块对应管理一个集群的所有日志信息。

#### 解决方案
在kubernetes集群中一般使用前文提到的第一种方式，使用日志记录代理收集日志，再进行处理、展示。最常应用的是EFK日志解决方案，所谓的EFK分别代表Elasticsearch、Fluentd、Kibana三种工具
<img width="1166" alt="10" src="https://img-blog.csdnimg.cn/20190104142451394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25ldHdvcmtlbg==,size_16,color_FFFFFF,t_70">


- Fluentd主要功能是日志收集客户端，主要用途是收集每一个pod的日志，同时也会收集操作系统层面的日志，比如kubelet组件、docker服务、apiserver等组件的日志

- Elasticsearch是日志存储功能，主要用来存储收集来的日志生成索引

- Kibana主要用途是日志展示和管理搜索等。将ES集群中的日志索引通过图形界面展示出来，可以实时查看日志的内容和根据关键字搜索日志

我们为用户提供日志模块，为用户在集群中安装EFK解决方案，把Kibana模块集成到网页中展示日志，为用户提供配置页与ES和Fluentd集成，以供用户自主选择想要收集的日志信息，处理的具体配置（时间、数量等）。

#### 难点
- 三个应用的安装及默认配置，主要是日志收集工具Fluentd的配置，包括：
  - 选择要收集的日志
  - 索引文件的设置
  - 本地缓存的设置
  - 与ES的连接
  - 端口的配置

- 在保证默认安装配置可行的情况下还要允许用户自定义配置（optional）
- 当前集群日志管理工具的检测
  - 若已用日志管理工具且与DOP平台使用的工具冲突，则提示用户，卸载原工具。
  - 自动化卸载(optioinal)

- 编写配置、管理页面并与EFK连通
- 将Kibana集成到网页中

#### 参考： 

*https://blog.csdn.net/bbwangj/article/details/81977947
 https://blog.csdn.net/weixin_34387468/article/details/92761145*






### 时间安排

- 第一迭代：
  - 11月-12月初     带领学弟熟悉
  - 部署熟悉V1，技术调研
  - 填上v1版本的坑

- 第二迭代

  - 12月- 1月20号（腊月26）
  - 需求收集、初始阶段开发
  - 本机开发

  - 尝试性接入dop平台、监控用起来

- 第三阶段 
  - 2月--4月
  - 完善一些策略

- 第四阶段（时间预留）
  - 4月--5月
  - bug修复



