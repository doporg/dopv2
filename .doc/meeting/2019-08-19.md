除了遗留工作，v2中可以考虑扩展的方向：测试自服务平台、监控及告警（向AIOps靠拢）

# Hoverfly
Hoverfly是一个轻量的API服务模拟工具（有时候也被称作服务虚拟化工具）。 使用Hoverfly，您可以创建应用程序依赖的API的真实模拟（可以捕获/模拟实际服务）
Hoverfly的特征：

- 创建可重复使用的虚拟服务，在CI环境中替代缓慢和不稳定的外部或第三方服务
- 模拟网络延迟，随机故障或速率限制以测试边缘情况
- 使用多种编程语言扩展和自定义， 包括Go，Java，Javascript，Python
- 导出，共享，编辑和导入API模拟数据
- Java和Python的语言绑定
- REST API
- 轻巧，高性能，随处运行
- 采用 Apache 2许可证


## Download and installation
### Docker
```
docker run -d -p 8888:8888 -p 8500:8500 spectolabs/hoverfly:latest
```
这将运行的最新版本的  [Hoverfly Docker image](https://hub.docker.com/r/spectolabs/hoverfly/).

从Docker开始时，还可以传递Hoverfly配置标志。例如，如果你需要在webserver模式下运行Hoverfly:
```
docker run -d -p 8888:8888 -p 8500:8500 spectolabs/hoverfly:latest -webserver
```
此Docker映像不包含hoverctl。我们的建议是在您的主机上安装hoverctl，然后将hoverctl配置为使用新启动的Hoverfly Docker实例作为新目标。

### windows
-   [Windows 32bit](https://github.com/SpectoLabs/hoverfly/releases/download/v1.1.1/hoverfly_bundle_windows_386.zip)
-   [Windows 64bit](https://github.com/SpectoLabs/hoverfly/releases/download/v1.1.1/hoverfly_bundle_windows_amd64.zip)

下载之后解压放入环境变量中(不管用)，放入GOPATH下运行

Once you have extracted both Hoverfly and hoverctl into a directory on your PATH, you can run hoverctl and Hoverfly.
```
hoverctl version
hoverfly -version
```

这两个命令都应该返回版本号。现在你可以运行一个Hoverfly的实例:
```
hoverctl start
```

使用以下命令检查Hoverfly是否正在运行:
```
hoverctl logs
```
日志应该包含字符串“服务代理”。这表明Hoverfly正在运行。
最后，停止Hoverfly:

```
hoverctl stop
```

### Hoverfly modes
#### Capture mode（捕获模式）
![capture mermaid](https://user-images.githubusercontent.com/17808702/63153602-4a17ed00-c041-11e9-8295-54f688d58783.png)
Hoverfly处于捕获模式 - 作为实际服务的代理服务



####  Simulate mode（模拟模式）
![simulate mermaid](https://user-images.githubusercontent.com/17808702/63153853-e3470380-c041-11e9-86b8-ad2647b4563a.png)

创建模拟请求如下：[Hoverfly- Creating and exporting a simulation](https://hoverfly.readthedocs.io/en/latest/pages/tutorials/basic/exportingsimulations/exportingsimulations.html)

#### Spy mode（侦察模式）
在这种模式下，如果在模拟数据中发现请求匹配，Hoverfly将模拟外部API，否则，请求将被传递到实际API。
**有中间件先调用中间件，并不会调用.json**

####  Synthesize mode（合成模式）
![synthesize mermaid](https://user-images.githubusercontent.com/17808702/63154929-38841480-c044-11e9-8f8a-36bb643ee8fc.png)

这种模式类似于模拟模式，但不是在存储的模拟数据中寻找响应，而是将请求直接传递给用户提供的可执行文件。这些文件称为**中间件**。

创建模拟请求如下：[Using middleware to modify response payload and status code](https://hoverfly.readthedocs.io/en/latest/pages/tutorials/basic/modifyingresponses/modifyingresponses.html)

####  Modify mode（修改模式）
![modify mermaid](https://user-images.githubusercontent.com/17808702/63155438-2060c500-c045-11e9-94e3-efb2fd2263ea.png)

修改模式类似于捕获模式，只是它不保存请求和响应。在修改模式下，Hoverfly将把每个请求传递给**中间件**可执行文件，然后再将其转发到目标。响应还将在返回给客户机之前传递给中间件。

#### Diff mode（差异模式)

在这种模式下，Hoverfly将请求转发给外部服务，并将响应与当前存储的模拟进行比较。通过存储模拟响应和外部服务的真实响应，Hoverfly能够检测两者之间的差异。当Hoverfly完成对两个响应的比较后，将存储差异，并将传入的请求作为来自外部服务的真实响应提供服务。

可以使用API (GET /api/v2/diff)从Hoverfly检索差异。响应包含差异列表，其中包含请求和响应的差异。
```
{
  "diff": [{
    "request": {
      "method": "GET",
      "host": "time.jsontest.com",
      "path": "/",
      "query": ""
    },
    "diffReports": [{
      "timestamp": "2018-03-16T17:45:40Z",
      "diffEntries": [{
        "field": "header/X-Cloud-Trace-Context",
        "expected": "[ec6c455330b682c3038ba365ade6652a]",
        "actual": "[043c9bb2eafa1974bc09af654ef15dc3]"
      }, {
        "field": "header/Date",
        "expected": "[Fri, 16 Mar 2018 17:45:34 GMT]",
        "actual": "[Fri, 16 Mar 2018 17:45:41 GMT]"
      }, {
        "field": "body/time",
        "expected": "05:45:34 PM",
        "actual": "05:45:41 PM"
      }, {
        "field": "body/milliseconds_since_epoch",
        "expected": "1.521222334104e+12",
        "actual": "1.521222341017e+12"
      }]
    }]
  }]
}
```
# Service Mesh
服务网格是一个基础设施层，功能在于处理服务间通信，职责是负责实现请求的可靠传递。在实践中，服务网格通常实现为轻量级网络代理，通常与应用程序部署在一起，但是对应用程序透明。

## 三种服务发现模式

### 传统集中式代理
![p](https://user-images.githubusercontent.com/17808702/63208050-5912a400-c102-11e9-8758-5f80d708a0ef.jpg)
这是最简单和传统做法，在服务消费者和生产者之间，代理作为独立一层集中部署，由独立团队 (一般是运维或框架) 负责治理和运维。常用的集中式代理有硬件负载均衡器 (如 F5)，或者软件负载均衡器 (如 Nginx)，F5(4 层负载)+Nginx(7 层负载) 这种软硬结合两层代理也是业内常见做法，兼顾配置的灵活性 (Nginx 比 F5 易于配置)。  

这种方式通常在 DNS 域名服务器的配合下实现服务发现，服务注册 (建立服务域名和 IP 地址之间的映射关系) 一般由运维人员在代理上手工配置，服务消费方仅依赖服务域名，这个域名指向代理，由代理解析目标地址并做负载均衡和调用。

### 客户端嵌入式代理
![p](https://user-images.githubusercontent.com/17808702/63208068-9bd47c00-c102-11e9-8e67-dd7396ac0964.jpg)
这是很多互联网公司比较流行的一种做法，代理 (包括服务发现和负载均衡逻辑) 以客户库的形式嵌入在应用程序中。这种模式一般需要独立的服务注册中心组件配合，服务启动时自动注册到注册中心并定期报心跳，客户端代理则发现服务并做负载均衡。

### 主机独立进程代理
![p](https://user-images.githubusercontent.com/17808702/63208087-c0c8ef00-c102-11e9-809b-3f1cba4f7b28.jpg)
这种做法是上面两种模式的一个折中，代理既不是独立集中部署，也不嵌入在客户应用程序中，而是作为独立进程部署在每一个主机上，一个主机上的多个消费者应用可以共用这个代理，实现服务发现和负载均衡，如下图所示。这个模式一般也需要独立的服务注册中心组件配合，作用同模式二。
### 服务网格ServiceMesh
![p](https://user-images.githubusercontent.com/17808702/63208130-26b57680-c103-11e9-9a34-7cd802ad1d9b.jpg)
服务的消费方和提供方主机 (或者容器) 两边都会部署代理 SideCar。ServiceMesh 比较正式的术语也叫数据平面 (DataPlane)，与数据平面对应的还有一个独立部署的控制平面 (ControlPlane)，用来集中配置和管理数据平面，也可以对接各种服务发现机制 (如 K8S 服务发现)。
每个主机上同时居住了业务逻辑代码 (绿色表示) 和代理 (蓝色表示)，服务之间通过代理发现和调用目标服务，形成服务之间的一种网络状依赖关系，控制平面则可以配置这种依赖调用关系，也可以调拨路由流量。如果我们把主机和业务逻辑剥离，就出现一种网格状架构 (上图右下角)，服务网格由此得名。

Istio是 Google/IBM 等大厂支持和推进的一个 ServiceMesh 标准化工作组，Istio 专注在控制平面的架构、功能、以及控制平面和数据平面之间 API 的标准化。
###  建议
- 本质上，ServiceMesh 其实并不是新东西，它只是模式三主机独立进程模式，这个模式早就有公司在探索和实践了，但是并未流行起来，可见这个模式也是存在落地挑战的。 表面上看，模式三既是模式一和模式二的折中，也解决了模式一和模式二存在的问题。但是在每个主机上独立部署一个代理进程，是有很大运维管理开销的，一方面是规模化部署的问题 (考虑服务很多，机器也很多的场景)；另一方面是如何监控治理的问题，代理挂了怎么办？你的团队是否具备自动化运维和监控的能力？另外开发人员在服务调试的时候，会依赖于这个独立的代理，调试排错比较麻烦，这个问题怎么解决？
 - Istio 的确做了一些标准化工作，但是没有什么特别的创新，可是说换汤不换药，就是把模式三规范化和包装了一下。透过现象看本质，Google/IBM 等行业大厂在背后推 Isito/ServiceMesh，背后有一些市场利益诉求考虑，例如 Google 要推进它的 kubernates 和公有云生态。
- 到国内只有一些大厂 (华为，新浪微博，蚂蚁金服等) 在试水，实际生产级落地的案例聊聊无几。大多数企业对 ServiceMesh 只是观望，很多架构师对 ServiceMesh 实际落地都存在疑虑。

# Prometheus&Grafana
Prometheus是一个最初在SoundCloud上构建的开源系统监视和警报工具包 。自2012年成立以来，许多公司和组织都采用了Prometheus，该项目拥有一个非常活跃的开发人员和用户社区。

## 下载安装（docker）

### Prometheus下载
你可以使用`docker pull`命令来下载Prometheus docker image
```
$ docker pull prom/prometheus
```
### Prometheus配置
创建一个`prometheus.yml`的文件，填入以下内容：
```
# my global config
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
alerting:
  alertmanagers:
  - static_configs:
    - targets:
      # - alertmanager:9093

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'prometheus'

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
    - targets: ['localhost:9182']

  - job_name: 'actuator-demo'
    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.
    metrics_path: '/actuator/prometheus'
    static_configs:
    - targets: ['localhost:9080']
```
### 使用Docker运行Prometheus
```
$ docker run -d --name=prometheus -p 9090:9090 -v <PATH_TO_prometheus.yml_FILE>:/etc/prometheus/prometheus.yml prom/prometheus 

```
确保替换<PATH_TO_prometheus.yml_FILE>为你在上面创建的Prometheus配置文件的保存的路径。

### Grafana下载
使用以下命令可以使Docker下载和运行Grafana：
```
$ docker run -d --name=grafana -p 3000:3000 grafana/grafana 
```
## springboot中使用

### pom配置
```
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
<dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-registry-prometheus</artifactId>
</dependency>
```
### 项目开启监控
在`application.properties`中添加：
```
management.endpoint.metrics.enabled=true
management.endpoints.web.exposure.include=*
management.endpoint.prometheus.enabled=true
management.metrics.export.prometheus.enabled=true
```

### 添加job
打开prometheus.yml，新增节点：
```
- job_name: xeblog-api
    metrics_path: /actuator/prometheus
    static_configs:
    - targets: ['127.0.0.1:8080’]
```
job_name：任务名称  
metrics_path： 指标路径  
targets：实例地址/项目地址，可配置多个

### 一些例子

- 系统cpu使用
system_cpu_usage
![TIM截图20190818201207](https://user-images.githubusercontent.com/17808702/63224438-cd2e7400-c1f6-11e9-8ca4-3843329a44d0.png)

- API响应延迟
http_server_requests_seconds_max{uri="/"}
![TIM截图20190818202715](https://user-images.githubusercontent.com/17808702/63224447-f0592380-c1f6-11e9-9c9a-a9645c88acf8.png)

- API响应次数
http_server_requests_seconds_count
![TIM截图20190818202650](https://user-images.githubusercontent.com/17808702/63224442-e0d9da80-c1f6-11e9-9869-4d56a92427e8.png)
